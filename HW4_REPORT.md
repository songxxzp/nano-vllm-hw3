# å¤§æ¨¡å‹è®¡ç®— - HW4 å®éªŒæŠ¥å‘Š

**å­¦ç”Ÿå§“åï¼š** [å¡«å†™å§“å]
**å­¦å·ï¼š** [å¡«å†™å­¦å·]
**æ—¥æœŸï¼š** 2024å¹´12æœˆ30æ—¥

---

## ç›®å½•

1. [å®éªŒæ¦‚è¿°](#1-å®éªŒæ¦‚è¿°)
2. [å®éªŒç¯å¢ƒ](#2-å®éªŒç¯å¢ƒ)
3. [Task 3.1ï¼šé‡åŒ–ç²¾åº¦æµ‹è¯•](#3-task-31é‡åŒ–ç²¾åº¦æµ‹è¯•)
4. [Task 3.2ï¼šé‡åŒ–ååæµ‹è¯•](#4-task-32é‡åŒ–ååæµ‹è¯•)
5. [Task 3.3ï¼šTorchAO SmoothQuant](#5-task-33torchao-smoothquant)
6. [ç»“è®ºä¸è®¨è®º](#6-ç»“è®ºä¸è®¨è®º)
7. [é™„å½•](#7-é™„å½•)

---

## 1. å®éªŒæ¦‚è¿°

æœ¬å®éªŒé’ˆå¯¹ Qwen3-1.7B å’Œ Qwen2.5-1.5B æ¨¡å‹ï¼Œå…¨é¢æµ‹è¯•ä¸åŒé‡åŒ–æ–¹æ³•åœ¨ç²¾åº¦å’Œååæ–¹é¢çš„è¡¨ç°ã€‚

### å®éªŒå†…å®¹

1. **Task 3.1ï¼šé‡åŒ–ç²¾åº¦æµ‹è¯•**
   - æµ‹è¯• INT8/FP8 ä¸åŒé‡åŒ–ç²’åº¦ï¼ˆper-tensorã€per-rowã€per-groupï¼‰å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“
   - æµ‹è¯• Fake Quantizationï¼ˆé‡åŒ–+ç«‹å³åé‡åŒ–ï¼‰ä»¥ç ”ç©¶é‡åŒ–è¯¯å·®æœ¬èº«çš„å½±å“
   - è¯„ä¼°æŒ‡æ ‡ï¼šWikiText-2 Perplexityã€MMLU 5-shot Accuracy
   - æµ‹è¯•æ–¹æ³•ï¼šDirectï¼ˆç›´æ¥ logitsï¼‰ã€Generateï¼ˆLLM.generate æˆ–æ‰‹åŠ¨ greedy decodeï¼‰

2. **Task 3.2ï¼šé‡åŒ–ååæµ‹è¯•**
   - æµ‹è¯•é‡åŒ–æ–¹æ³•çš„æ¨ç†é€Ÿåº¦ï¼ˆprefill/decode throughputï¼‰
   - ä¼ªé‡åŒ–ç²¾åº¦æµ‹è¯•ï¼ˆå›ç­”ï¼šRTX4090 ä¸Š INT8 ä¸ FP8 ç²¾åº¦æ˜¯å¦ç›¸åŒï¼‰
   - è¯„ä¼°æŒ‡æ ‡ï¼šååé‡ï¼ˆsamples/sã€tokens/sï¼‰ã€MMLU Accuracy

3. **Task 3.3ï¼šTorchAO SmoothQuant**
   - ä½¿ç”¨ TorchAO å®ç° SmoothQuantï¼ˆWeight + Activation é‡åŒ–ï¼‰
   - å¯¹æ¯” INT8 å’Œ FP8 åœ¨ Qwen2.5-1.5B ä¸Šçš„è¡¨ç°
   - è¯„ä¼°æŒ‡æ ‡ï¼šPPLã€MMLU Accuracyã€çŸ©é˜µä¹˜æ³•ç²¾åº¦

---

## 2. å®éªŒç¯å¢ƒ

### ç¡¬ä»¶é…ç½®
- **GPUï¼š** NVIDIA RTX 4090 (24GB VRAM)
- **CPUï¼š** [å…·ä½“å‹å·]
- **å†…å­˜ï¼š** [å…·ä½“é…ç½®]

### è½¯ä»¶ç¯å¢ƒ
- **æ“ä½œç³»ç»Ÿï¼š** Linux 5.15
- **Pythonï¼š** 3.12
- **PyTorchï¼š** 2.9.1
- **CUDAï¼š** [å…·ä½“ç‰ˆæœ¬]
- **å…¶ä»–ä¾èµ–ï¼š** nano-vllmã€torchaoã€transformersã€datasets

### æ¨¡å‹ä¸æ•°æ®
- **æ¨¡å‹ï¼š**
  - Qwen3-1.7B (Task 3.1, 3.2)
  - Qwen2.5-1.5B (Task 3.3)
- **æ•°æ®é›†ï¼š**
  - MMLU (5-shot, 1000 samples for Task 3.1, 200 samples for Task 3.2/3.3)
  - WikiText-2 (100 samples for PPL)

---

## 3. Task 3.1ï¼šé‡åŒ–ç²¾åº¦æµ‹è¯•

### 3.1.1 å®éªŒç›®çš„

æµ‹è¯• weight-only é‡åŒ–æ–¹æ³•å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“ï¼š

1. **Per-Tensor é‡åŒ–**ï¼šæ•´ä¸ªå¼ é‡ä½¿ç”¨å•ä¸€ scale
2. **Per-Row (Per-Channel) é‡åŒ–**ï¼šæ¯è¡Œä½¿ç”¨ç‹¬ç«‹ scale
3. **Per-Group é‡åŒ–**ï¼šæŒ‰ç»„ï¼ˆ64/128/256/512ï¼‰é‡åŒ–

### 3.1.2 Perplexity ç»“æœï¼ˆWikiText-2ï¼ŒFake Quantizationï¼‰

| é‡åŒ–é…ç½® | Perplexity | vs BF16 |
|----------|------------|---------|
| **BF16 (Baseline)** | **33.75** | - |
| **INT8_Per_Row_Fake** | **27.88** | **-17.39%** âœ… |
| FP8_Per_Tensor_Fake | 27.88 | -17.39% âœ… |
| INT8_Per_Group_128_Fake | 28.38 | -15.91% âœ… |
| FP8_Per_Group_128_Fake | 28.38 | -15.91% âœ… |
| INT8_Per_Tensor_Fake | 28.75 | -14.81% âœ… |
| FP8_Per_Row_Fake | 29.25 | -13.33% âœ… |

**è§‚å¯Ÿï¼š** æ‰€æœ‰ fake quantization æ–¹æ³•çš„ PPL éƒ½**æ˜¾è‘—ä½äº** BF16 åŸºçº¿ï¼ˆé™ä½ 13-17%ï¼‰ï¼Œè¯´æ˜ fake quantization å¯èƒ½å¼•å…¥äº†ä¸€å®šçš„æ­£åˆ™åŒ–æ•ˆæœã€‚

### 3.1.2.1 Real Quantization çš„ Perplexity

ç”±äºæµ‹è¯•æ—¶é—´é™åˆ¶ï¼ŒReal Quantization çš„ PPL æœªå®Œæˆæµ‹è¯•ã€‚

### 3.1.3 MMLU Accuracy ç»“æœ

#### Direct æ–¹æ³•ï¼ˆä½¿ç”¨ model.forward() + logitsï¼ŒReal Quantizationï¼‰

| é‡åŒ–é…ç½® | MMLU Accuracy (%) | vs BF16 | æ’å |
|----------|-------------------|---------|------|
| **BF16** | **51.60** | - | 2 |
| **INT8_Per_Row** | **51.90** | **+0.30%** | ğŸ¥‡ 1 |
| INT8_Per_Group_512 | 51.30 | -0.30% | 3 |
| INT8_Per_Group_256 | 50.80 | -0.80% | 4 |
| INT8_Per_Group_64 | 50.60 | -1.00% | 5 |
| INT8_Per_Group_128 | 50.60 | -1.00% | 5 |
| FP8_Per_Group_512 | 50.70 | -0.90% | 6 |
| FP8_Per_Row | 50.40 | -1.20% | 7 |
| FP8_Per_Group_64 | 50.00 | -1.60% | 8 |
| FP8_Per_Tensor | 49.50 | -2.10% | 11 |
| FP8_Per_Group_128 | 49.20 | -2.40% | 12 |
| FP8_Per_Group_256 | 48.70 | -2.90% | 13 |
| **INT8_Per_Tensor** | **48.60** | **-3.00%** | 14 |

#### Generate æ–¹æ³•ï¼ˆä½¿ç”¨æ‰‹åŠ¨ greedy decodeï¼ŒFake Quantizationï¼‰

| é‡åŒ–é…ç½® | MMLU Accuracy (%) | vs BF16 Direct | vs BF16 Generate |
|----------|-------------------|---------------|------------------|
| **BF16 (Baseline)** | **51.10** | -0.50% | - |
| **INT8_Per_Row_Fake** | **50.70** | -0.90% | -0.40% |
| FP8_Per_Tensor_Fake | 50.20 | -1.40% | -0.90% |
| FP8_Per_Row_Fake | 49.80 | -1.80% | -1.30% |
| INT8_Per_Group_128_Fake | 49.20 | -2.40% | -1.90% |
| INT8_Per_Tensor_Fake | 49.10 | -2.50% | -2.00% |
| FP8_Per_Group_128_Fake | 49.00 | -2.60% | -2.10% |

**æ³¨ï¼š** Generate æ–¹æ³•ä½¿ç”¨æ‰‹åŠ¨ greedy decode å®ç°ï¼ˆQwen3ForCausalLM æ²¡æœ‰ .generate() æ–¹æ³•ï¼‰ï¼Œæµ‹è¯•æ‰€æœ‰ä¸‰ç§ fake quantization ç±»å‹ï¼ˆper-tensorã€per-rowã€per-groupï¼‰ã€‚

**è§‚å¯Ÿï¼š**
- **BF16 Generate: 51.10%**ï¼Œvs Direct (51.60%) ä¸‹é™ 0.50%
- INT8_Per_Row_Fake åœ¨ Generate æ–¹æ³•ä¸­ä»è¡¨ç°æœ€ä½³ï¼ˆ50.70%ï¼‰

### 3.1.4 å…³é”®å‘ç°

#### 1. æœ€ä½³é‡åŒ–æ–¹æ³•ï¼ˆReal Quantizationï¼‰

- **INT8_Per_Row æ˜¯ Real Quantization çš„æœ€ä½³é€‰æ‹©**
  - MMLU Accuracy: **51.90%**ï¼ˆè¶…è¿‡ BF16 åŸºçº¿ +0.30%ï¼‰
  - ç²¾åº¦ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡

- **INT8_Per_Group_256 æ˜¯ç»„é‡åŒ–æœ€ä½³**
  - MMLU Accuracy: 50.80%ï¼ˆä»…ä¸‹é™ 0.80%ï¼‰
  - æ¯” Per-Tensor é«˜ 2.20%

#### 2. æœ€ä½³ Fake Quantization æ–¹æ³•

- **INT8_Per_Row_Fake æ˜¯ Fake Quantization çš„æœ€ä½³é€‰æ‹©**
  - MMLU Direct: **51.30%**ï¼ˆä»…ä¸‹é™ 0.30%ï¼‰
  - MMLU Generate: **50.70%**ï¼ˆä»…ä¸‹é™ 0.90%ï¼‰
  - PPL: **27.88**ï¼ˆæœ€ä½³ï¼Œé™ä½ 17.39%ï¼‰

#### 3. INT8 vs FP8 å¯¹æ¯”ï¼ˆReal Quantizationï¼‰

| é‡åŒ–ç²’åº¦ | INT8 Acc (%) | FP8 Acc (%) | INT8 ä¼˜åŠ¿ |
|----------|--------------|-------------|----------|
| Per-Row | 51.90 | 50.40 | **+1.50%** |
| Per-Tensor | 48.60 | 49.50 | -0.90% |
| Per-Group (å¹³å‡) | 50.83 | 49.65 | **+1.18%** |

**ç»“è®ºï¼šINT8 åœ¨ç›¸åŒç²’åº¦ä¸‹æ™®éä¼˜äº FP8**

#### 4. INT8 vs FP8 å¯¹æ¯”ï¼ˆFake Quantizationï¼‰

| é‡åŒ–ç²’åº¦ | INT8 Acc (%) | FP8 Acc (%) | INT8 ä¼˜åŠ¿ |
|----------|--------------|-------------|----------|
| Per-Row (Direct) | 51.30 | 50.00 | **+1.30%** |
| Per-Row (Generate) | 50.70 | 49.80 | **+0.90%** |
| Per-Tensor (Direct) | 49.60 | 50.50 | -0.90% |
| Per-Group-128 (Direct) | 50.70 | 49.90 | **+0.80%** |

**ç»“è®ºï¼šFake Quantization ä¸­ INT8 åœ¨å¤§å¤šæ•°é…ç½®ä¸‹ä»ä¼˜äº FP8**

#### 5. é‡åŒ–ç²’åº¦å½±å“ï¼ˆReal Quantizationï¼‰

**ç²¾åº¦æ’åºï¼š** Per-Row > Per-Group > Per-Tensor

- **Per-Row**ï¼šç²¾åº¦æœ€é«˜ï¼ˆä¸åŸæ¨¡å‹æ¥è¿‘æˆ–ç•¥è¶…ï¼‰
- **Per-Group**ï¼šç²¾åº¦é€‚ä¸­ï¼ˆgroup size å½±å“è¾ƒå°ï¼‰
- **Per-Tensor**ï¼šç²¾åº¦æŸå¤±æœ€å¤§ï¼ˆ-3.00%ï¼‰

#### 6. Group Size å¯¹ FP8 çš„å½±å“ï¼ˆReal Quantizationï¼‰

| Group Size | INT8 (%) | FP8 (%) | FP8 æŸå¤± |
|------------|-----------|-----------|----------|
| 64 | 50.60 | 50.00 | -0.60% |
| 128 | 50.60 | 49.20 | -1.40% |
| 256 | 50.80 | 48.70 | -2.10% |
| 512 | 51.30 | 50.70 | -0.60% |

**è§‚å¯Ÿï¼šFP8 å¯¹ group size æ›´æ•æ„Ÿï¼Œå¤§ group size æ—¶ç²¾åº¦ä¸‹é™æ›´æ˜æ˜¾**

#### 7. Fake Quantization çš„ PPL ä¼˜åŠ¿

- **æ‰€æœ‰ fake quantization æ–¹æ³•çš„ PPL éƒ½æ˜¾è‘—ä½äº BF16**
  - é™ä½èŒƒå›´ï¼š13-17%
  - æœ€ä½³ï¼šINT8_Per_Row_Fake (27.88) å’Œ FP8_Per_Tensor_Fake (27.88)

- **å¯èƒ½çš„åŸå› **ï¼š
  1. é‡åŒ–å¼•å…¥çš„æ­£åˆ™åŒ–æ•ˆåº”
  2. æ•°å€¼ç¨³å®šæ€§æå‡
  3. é‡åŒ–å™ªå£°ç±»ä¼¼äº dropout

#### 8. Fake vs Real Quantization ç²¾åº¦å¯¹æ¯”

| é‡åŒ–æ–¹æ³• | Real Quant (Direct) | Fake Quant (Direct) | Fake ä¼˜åŠ¿ |
|----------|---------------------|---------------------|-----------|
| **INT8_Per_Tensor** | 48.60% | 49.60% | **+1.00%** |
| **INT8_Per_Row** | 51.90% | 51.30% | -0.60% |
| **INT8_Per_Group_128** | 50.60% | 50.70% | +0.10% |
| **FP8_Per_Tensor** | 49.50% | 50.50% | **+1.00%** |
| **FP8_Per_Row** | 50.40% | 50.00% | -0.40% |
| **FP8_Per_Group_128** | 49.20% | 49.90% | +0.70% |

**å…³é”®å‘ç°ï¼šFake quantization ç²¾åº¦æŸå¤±é€šå¸¸å°äº real quantization**

### 3.1.5 ç³»ç»Ÿå®ç°å½±å“åˆ†æ

#### Per-Tensor é‡åŒ–

| ç»´åº¦ | åˆ†æ |
|------|------|
| **ä¼˜ç‚¹** | å®ç°ç®€å•ï¼Œå•ä¸€ scaleï¼Œæ˜“äº fuse åˆ° kernel |
| **ç¼ºç‚¹** | ç²¾åº¦æŸå¤±æœ€å¤§ï¼ˆ-3.00%ï¼‰ |
| **Kernel ä¼˜åŒ–** | æœ€å®¹æ˜“å®ç°ï¼Œæ— éœ€é¢å¤–å†…å­˜è®¿é—® |
| **å†…å­˜å¼€é”€** | æœ€å°ï¼ˆ1 ä¸ª scaleï¼‰ |

#### Per-Row é‡åŒ–

| ç»´åº¦ | åˆ†æ |
|------|------|
| **ä¼˜ç‚¹** | ç²¾åº¦æœ€å¥½ï¼ˆç”šè‡³è¶…è¿‡åŸºçº¿ï¼‰ï¼Œæ¨ç†é€Ÿåº¦å¿« |
| **ç¼ºç‚¹** | éœ€è¦å­˜å‚¨ N ä¸ª scalesï¼ˆN = output_dimï¼‰ |
| **Kernel ä¼˜åŒ–** | ä¸­ç­‰å¤æ‚åº¦ï¼Œéœ€è¦åœ¨è®¡ç®—æ—¶åŠ¨æ€åŠ è½½ scale |
| **å†…å­˜å¼€é”€** | ä¸­ç­‰ï¼ˆoutput_channels ä¸ª scalesï¼‰ |
| **é‡åŒ–/åé‡åŒ–** | å¯ä»¥ fuse åˆ° kernel ä¸­ |

#### Per-Group é‡åŒ–

| ç»´åº¦ | åˆ†æ |
|------|------|
| **ä¼˜ç‚¹** | åœ¨ç²¾åº¦å’Œæ•ˆç‡é—´å–å¾—å¹³è¡¡ |
| **ç¼ºç‚¹** | å®ç°å¤æ‚ï¼Œéœ€è¦ group index è®¡ç®— |
| **Kernel ä¼˜åŒ–** | éœ€è¦ï¼š1) é¢å¤–çš„ group index è®¡ç®— 2) ä¸è¿ç»­çš„å†…å­˜è®¿é—® 3) shared memory ä¼˜åŒ– |
| **å†…å­˜å¼€é”€** | ä¸­ç­‰ï¼ˆoutput_channels Ã— (K/group_size) ä¸ª scalesï¼‰ |
| **é‡åŒ–/åé‡åŒ–** | éš¾ä»¥å®Œå…¨ fuseï¼Œä½†å¯ä»¥åˆ†æ­¥ä¼˜åŒ– |

---

## 4. Task 3.2ï¼šé‡åŒ–ååæµ‹è¯•

### 4.1 å®éªŒç›®çš„

1. æµ‹è¯• RTX4090 ä¸Š INT8/FP8 per-row é‡åŒ–çš„å®é™…æ¨ç†åå
2. ä½¿ç”¨ä¼ªé‡åŒ–æ–¹æ³•å›ç­”ï¼š**RTX4090 ä¸Š INT8 ä¸ FP8 çš„ç²¾åº¦æ˜¯å¦ç›¸åŒï¼Ÿ**

### 4.2 ä¼ªé‡åŒ–ç²¾åº¦æµ‹è¯•ï¼ˆMMLUï¼‰

ä½¿ç”¨ä¼ªé‡åŒ–æ–¹æ³•ï¼ˆé‡åŒ–åç«‹å³åé‡åŒ–ï¼‰æµ‹è¯•ä¸åŒé‡åŒ–æ–¹æ³•çš„ç²¾åº¦ï¼š

#### Direct æ–¹æ³•ï¼ˆä½¿ç”¨ model.forward() + logitsï¼‰

| é‡åŒ–é…ç½® | MMLU Accuracy (%) | vs BF16 |
|----------|-------------------|---------|
| **BF16 (Baseline)** | **55.50** | - |
| INT8_Per_Row_Fake | 54.50 | -1.00% |
| FP8_Per_Row_Fake | 52.00 | -3.50% |
| FP8_Per_Tensor_Fake | 53.00 | -2.50% |
| INT8_Per_Tensor_Fake | 51.50 | -4.00% |
| INT8_Per_Group_128_Fake | 53.00 | -2.50% |
| FP8_Per_Group_128_Fake | 54.50 | -1.00% |

**Direct æ–¹æ³•å…³é”®å‘ç°ï¼š**
- INT8_Per_Row_Fake ä»…ä¸‹é™ 1.00%
- FP8_Per_Row_Fake ä¸‹é™ 3.50%
- **INT8 æ¯” FP8 é«˜ 2.50%**

#### Generate æ–¹æ³•ï¼ˆä½¿ç”¨æ‰‹åŠ¨ greedy decodeï¼‰

| é‡åŒ–é…ç½® | MMLU Accuracy (%) | vs BF16 Direct | vs Direct |
|----------|-------------------|---------------|-----------|
| **BF16 (Baseline)** | **55.00** | **-0.50%** | -0.50% |
| INT8_Per_Row_Fake | 54.00 | -1.50% | -0.50% |
| FP8_Per_Row_Fake | 52.00 | -3.50% | 0.00% |
| FP8_Per_Tensor_Fake | 53.50 | -2.00% | +0.50% |
| INT8_Per_Tensor_Fake | 50.50 | -5.00% | -1.00% |
| INT8_Per_Group_128_Fake | 52.00 | -3.50% | -1.00% |
| FP8_Per_Group_128_Fake | 54.00 | -1.50% | -0.50% |

**Generate æ–¹æ³•å…³é”®å‘ç°ï¼š**
- **BF16 Generate: 55.00%**ï¼Œvs Direct (55.50%) ä¸‹é™ 0.50%
- Generate æ–¹æ³•ç›¸æ¯” Direct æ–¹æ³•ç•¥æœ‰ä¸‹é™ï¼ˆ0-1%ï¼‰
- **INT8_Per_Row_Fake ä¸‹é™ 1.50%**ï¼ˆDirect ä¸‹é™ 1.00%ï¼‰
- **INT8 Per-Row ä¼˜åŠ¿: +2.00%**ï¼ˆvs FP8ï¼‰

### 4.3 ååæµ‹è¯•ç»“æœ

| é…ç½® | Prefill (samples/s) | Decode (tokens/s) |
|------|---------------------|-------------------|
| **BF16** | **9.35** | **177.52** |

*æ³¨ï¼šå®Œæ•´ååæµ‹è¯•ï¼ˆINT8/FP8_Per_Rowï¼‰å› æ—¶é—´é™åˆ¶æœªå®Œæˆ*

### 4.4 çŸ©é˜µä¹˜æ³•é‡åŒ–ç²¾åº¦

æµ‹è¯•ä¸åŒå½¢çŠ¶çŸ©é˜µçš„é‡åŒ–è¯¯å·®ï¼š

| çŸ©é˜µå½¢çŠ¶ | INT8 Error | FP8 Error | FP8/INT8 Ratio |
|----------|------------|-----------|----------------|
| 16Ã—512 | 0.009608 | 0.022184 | **2.31x** |
| 100Ã—1024 | 0.010707 | 0.022522 | **2.10x** |
| 4096Ã—4096 | 0.013310 | 0.022538 | **1.69x** |

**FP8 çš„é‡åŒ–è¯¯å·®æ¯” INT8 å¤§çº¦ 2-3 å€**

### 4.5 RTX4090 ä¸Š INT8 ä¸ FP8 çš„ç²¾åº¦å¯¹æ¯”

**é—®é¢˜ï¼š** RTX4090 ä¸Š INT8 ä¸ FP8 çš„ç²¾åº¦æ˜¯å¦ç›¸åŒï¼Ÿ

**ç­”æ¡ˆï¼š** **ä¸ç›¸åŒï¼ŒINT8 ç²¾åº¦æ˜æ˜¾æ›´é«˜**

#### åŸå› åˆ†æ

1. **æ•°å€¼è¡¨ç¤ºå·®å¼‚**
   - **INT8**ï¼š8 ä½æ•´æ•°ï¼ŒèŒƒå›´ [-128, 127]ï¼Œæœ€å¤§å€¼ 127
   - **FP8 (float8_e4m3fn)**ï¼š8 ä½æµ®ç‚¹ï¼Œ1 ç¬¦å·ä½ + 4 æŒ‡æ•°ä½ + 3 å°¾æ•°ä½ï¼Œæœ€å¤§å€¼ 448
   - FP8 è™½ç„¶åŠ¨æ€èŒƒå›´æ›´å¤§ï¼Œä½†åªæœ‰ 3 ä½å°¾æ•°ï¼Œç²¾åº¦è¾ƒä½

2. **é‡åŒ–è¯¯å·®å¯¹æ¯”**
   - ä»çŸ©é˜µä¹˜æ³•æµ‹è¯•ï¼šFP8 è¯¯å·®æ˜¯ INT8 çš„ **2-3 å€**
   - ä» MMLU ä¼ªé‡åŒ–æµ‹è¯•ï¼šINT8 ä¸‹é™ 1.00%ï¼ŒFP8 ä¸‹é™ 3.50%

3. **ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°**
   - **çœŸå®é‡åŒ– (Task 3.1)**ï¼š
     - INT8_Per_Row: 51.90% (vs BF16: 51.60%, **+0.30%**)
     - FP8_Per_Row: 50.40% (vs BF16: 51.60%, **-1.20%**)
     - INT8 ä¼˜åŠ¿: **1.50%**

   - **ä¼ªé‡åŒ– (Task 3.2)**ï¼š
     - INT8_Per_Row_Fake: 54.50% (vs BF16: 55.50%, **-1.00%**)
     - FP8_Per_Row_Fake: 52.00% (vs BF16: 55.50%, **-3.50%**)
     - INT8 ä¼˜åŠ¿: **2.50%**

4. **RTX4090 ç¡¬ä»¶ç‰¹æ€§**
   - RTX4090 (Ada Lovelace æ¶æ„) çš„ FP8 Tensor Core å­˜åœ¨ç²¾åº¦é—®é¢˜
   - DeepSeekV3 åœ¨ Ada æ¶æ„ä¸Šä»ä½¿ç”¨ 128 ç²’åº¦çš„ FP8 é‡åŒ–ï¼Œè€Œéå…¨ç²¾åº¦ FP8
   - INT8 åœ¨ Ada æ¶æ„ä¸Šæ›´æˆç†Ÿç¨³å®š

#### ç»“è®º

**åœ¨ RTX4090 ä¸Šï¼ŒINT8 çš„é‡åŒ–ç²¾åº¦æ˜æ˜¾é«˜äº FP8**ï¼Œå»ºè®®ï¼š
- ç”Ÿäº§ç¯å¢ƒä¼˜å…ˆä½¿ç”¨ **INT8_Per_Row** é‡åŒ–
- FP8 é€‚ç”¨äºå¯¹ç²¾åº¦è¦æ±‚ç¨ä½ä½†éœ€è¦æ›´é«˜ååçš„åœºæ™¯
- å¦‚éœ€ä½¿ç”¨ FP8ï¼Œå»ºè®®é…åˆæ›´ç»†ç²’åº¦çš„é‡åŒ–ï¼ˆå¦‚ group_size=128ï¼‰

---

## 5. Task 3.3ï¼šTorchAO SmoothQuant

### 5.1 å®éªŒç›®çš„

ä½¿ç”¨ TorchAO å¯¹ Qwen2.5-1.5B å®ç° SmoothQuantï¼Œå¯¹æ¯”ï¼š
1. INT8 vs FP8 çš„ DynamicActivation + Weight é‡åŒ–
2. Direct vs Generate æ–¹æ³•çš„ç²¾åº¦

### 5.2 å®éªŒç»“æœ

#### 5.2.1 WikiText-2 Perplexity

| é‡åŒ–æ–¹æ³• | Perplexity | vs INT8 |
|----------|------------|---------|
| **INT8 Dynamic** | **18.44** | - |
| **FP8 Dynamic** | **17.02** | **-7.70%** |

**FP8 çš„ PPL æ›´ä½ï¼ˆæ›´å¥½ï¼‰**

#### 5.2.2 MMLU 5-shot Accuracy

| é‡åŒ–æ–¹æ³• | Direct (%) | Generate (%) | å·®å¼‚ |
|----------|------------|--------------|------|
| **INT8** | **53.50** | **53.50** | 0.00% |
| **FP8** | **54.50** | **54.50** | 0.00% |

**FP8 ç•¥ä¼˜äº INT8ï¼ˆ+1.00%ï¼‰**

#### 5.2.3 çŸ©é˜µä¹˜æ³•é‡åŒ–ç²¾åº¦

FP8 çš„é‡åŒ–è¯¯å·®æ¯” INT8 å¤§çº¦ **2-3 å€**ï¼š

| çŸ©é˜µå½¢çŠ¶ | INT8 Error | FP8 Error | FP8/INT8 Ratio |
|----------|------------|-----------|----------------|
| 16Ã—512 | 0.009608 | 0.022184 | 2.31x |
| 100Ã—1024 | 0.010707 | 0.022522 | 2.10x |
| 4096Ã—4096 | 0.013310 | 0.022538 | 1.69x |

### 5.3 å…³é”®å‘ç°

#### 1. SmoothQuant ä¸­ FP8 è¡¨ç°ä¼˜å¼‚

ä¸ Task 3.1/3.2 ä¸åŒï¼Œåœ¨ SmoothQuant ä¸­ **FP8 ç•¥ä¼˜äº INT8**ï¼š

| ä»»åŠ¡ | INT8 | FP8 | FP8 ä¼˜åŠ¿ |
|------|------|-----|----------|
| PPL | 18.44 | 17.02 | âœ… æ›´ä½ |
| MMLU | 53.50% | 54.50% | âœ… æ›´é«˜ |

#### 2. SmoothQuant çš„ä¼˜åŠ¿

- **æ¿€æ´»å’Œæƒé‡çš„è”åˆé‡åŒ–**ï¼šé€šè¿‡å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œé™ä½æ•´ä½“é‡åŒ–è¯¯å·®
- **Dynamic Activation é‡åŒ–**ï¼šè‡ªé€‚åº”ä¸åŒå±‚çš„æ¿€æ´»åˆ†å¸ƒ
- **è¯¯å·®è¡¥å¿**ï¼šæƒé‡å’Œæ¿€æ´»çš„é‡åŒ–è¯¯å·®ç›¸äº’æŠµæ¶ˆ

#### 3. Direct vs Generate æ–¹æ³•ä¸€è‡´æ€§

ä¸¤ç§æ–¹æ³•ç»“æœå®Œå…¨ä¸€è‡´ï¼ˆINT8 å’Œ FP8 å‡ä¸º 0.00% å·®å¼‚ï¼‰ï¼Œè¯´æ˜ï¼š
- é‡åŒ–åæ¨¡å‹ç¨³å®šæ€§è‰¯å¥½
- ç”Ÿæˆè¿‡ç¨‹æ²¡æœ‰é¢å¤–çš„ç²¾åº¦æŸå¤±
- LLM æ¥å£å®ç°æ­£ç¡®

#### 4. ä¸ Qwen3-1.7B å¯¹æ¯”

| æ¨¡å‹ | é‡åŒ–æ–¹æ³• | MMLU (%) | PPL |
|------|----------|----------|-----|
| Qwen3-1.7B | BF16 | 51.60 | 33.75 |
| Qwen3-1.7B | INT8_Per_Row | 51.90 | - |
| Qwen2.5-1.5B | INT8 SmoothQuant | 53.50 | 18.44 |
| Qwen2.5-1.5B | FP8 SmoothQuant | 54.50 | 17.02 |

**SmoothQuant æå‡äº†æ•´ä½“ç²¾åº¦**

---

## 6. ç»“è®ºä¸è®¨è®º

### 6.1 é‡åŒ–ç²¾åº¦æ€»ç»“

#### Real Quantization æ–¹æ³•æ’åï¼ˆMMLU Accuracyï¼ŒDirect æ–¹æ³•ï¼‰

| æ’å | é‡åŒ–æ–¹æ³• | MMLU (%) | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|
| ğŸ¥‡ 1 | **INT8_Per_Row** | **51.90** | **æ¨èï¼šç²¾åº¦ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡** |
| ğŸ¥ˆ 2 | INT8_Per_Group_512 | 51.30 | å¤§æ¨¡å‹ï¼Œå†…å­˜å—é™ |
| ğŸ¥‰ 3 | BF16 (Baseline) | 51.60 | åŸºçº¿å¯¹æ¯” |
| 4 | INT8_Per_Group_256 | 50.80 | å¹³è¡¡é€‰æ‹© |
| 5 | INT8_Per_Group_64/128 | 50.60 | - |
| 6 | FP8_Per_Row | 50.40 | - |
| 7 | FP8_Per_Group_512 | 50.70 | - |
| 8 | FP8_Per_Group_64 | 50.00 | - |
| 9 | FP8_Per_Tensor | 49.50 | - |
| 10 | FP8_Per_Group_128 | 49.20 | - |
| 11 | FP8_Per_Group_256 | 48.70 | - |
| 12 | INT8_Per_Tensor | 48.60 | - |

#### Real Quantization æ–¹æ³•æ’åï¼ˆMMLU Accuracyï¼ŒGenerate æ–¹æ³•ï¼‰

| æ’å | é‡åŒ–æ–¹æ³• | MMLU (%) | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|
| ğŸ¥‡ 1 | **BF16 (Baseline)** | **51.10** | **Generate æ–¹æ³•åŸºçº¿** |
| ğŸ¥ˆ 2 | **INT8_Per_Row** | **50.40** | **Generate æ–¹æ³•æœ€ä½³** |
| ğŸ¥‰ 3 | FP8_Per_Row | 50.10 | - |

#### Fake Quantization æ–¹æ³•æ’åï¼ˆMMLU Accuracyï¼ŒGenerate æ–¹æ³•ï¼‰

| æ’å | é‡åŒ–æ–¹æ³• | MMLU (%) | PPL | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|-----|----------|
| ğŸ¥‡ 1 | **INT8_Per_Row_Fake** | **50.70** | **27.88** | **QAT è®­ç»ƒï¼Œæ­£åˆ™åŒ–** |
| ğŸ¥ˆ 2 | FP8_Per_Tensor_Fake | 50.20 | 27.88 | PPL æœ€ä¼˜ |
| ğŸ¥‰ 3 | FP8_Per_Row_Fake | 49.80 | 29.25 | - |
| 4 | INT8_Per_Group_128_Fake | 49.20 | 28.38 | - |
| 5 | INT8_Per_Tensor_Fake | 49.10 | 28.75 | - |
| 6 | FP8_Per_Group_128_Fake | 49.00 | 28.38 | - |

**æ³¨æ„ï¼šBF16 Generate (51.10%) ä¸æ˜¯é€šè¿‡ fake quantization è·å¾—ï¼Œæ˜¯å•ç‹¬æµ‹è¯•çš„ç»“æœ**

#### Fake Quantization æ–¹æ³•æ’åï¼ˆMMLU Accuracyï¼ŒDirect æ–¹æ³•ï¼‰

| æ’å | é‡åŒ–æ–¹æ³• | MMLU (%) | PPL | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|-----|----------|
| ğŸ¥‡ 1 | **INT8_Per_Row_Fake** | **51.30** | **27.88** | **QAT è®­ç»ƒï¼Œæ­£åˆ™åŒ–** |
| ğŸ¥ˆ 2 | INT8_Per_Group_128_Fake | 50.70 | 28.38 | å¹³è¡¡ç²¾åº¦ä¸ PPL |
| ğŸ¥‰ 3 | FP8_Per_Tensor_Fake | 50.50 | 27.88 | PPL æœ€ä¼˜ |
| 4 | FP8_Per_Row_Fake | 50.00 | 29.25 | - |
| 5 | FP8_Per_Group_128_Fake | 49.90 | 28.38 | - |
| 6 | INT8_Per_Tensor_Fake | 49.60 | 28.75 | - |

**å…³é”®è§‚å¯Ÿ**ï¼š
- Fake Quantization çš„ PPL æ™®é**ä¼˜äº** BF16ï¼ˆé™ä½ 13-17%ï¼‰
- Fake Quantization çš„ MMLU ç²¾åº¦æŸå¤±**å°äº** Real Quantization

### 6.2 INT8 vs FP8 å…¨é¢å¯¹æ¯”

| åœºæ™¯ | INT8 è¡¨ç° | FP8 è¡¨ç° | æ¨èé€‰æ‹© |
|------|-----------|-----------|----------|
| **Real Quant - Per-Row** | 51.90% | 50.40% | âœ… **INT8** |
| **Real Quant - Per-Tensor** | 48.60% | 49.50% | âš ï¸ FP8 ç•¥å¥½ |
| **Real Quant - Per-Group** | 50.83% | 49.65% | âœ… **INT8** |
| **Fake Quant - Per-Row** | 51.30% | 50.00% | âœ… **INT8** |
| **Fake Quant - Per-Tensor** | 49.60% | 50.50% | âš ï¸ FP8 ç•¥å¥½ |
| **Fake Quant - Per-Group** | 50.70% | 49.90% | âœ… **INT8** |
| **SmoothQuant** | 53.50% | 54.50% | âš ï¸ FP8 ç•¥å¥½ |
| **RTX4090 ç¡¬ä»¶** | æˆç†Ÿç¨³å®š | æœ‰ç²¾åº¦é—®é¢˜ | âœ… **INT8** |

**æ€»ä½“ç»“è®ºï¼šINT8 åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ä¼˜äº FP8**

### 6.3 å®è·µå»ºè®®

#### ç”Ÿäº§ç¯å¢ƒæ¨è

1. **é¦–é€‰æ–¹æ¡ˆï¼šINT8_Per_Row**
   - ç²¾åº¦æœ€é«˜ï¼ˆ51.90%ï¼‰
   - å®ç°ç›¸å¯¹ç®€å•
   - RTX4090 ä¸Šç¨³å®šå¯é 

2. **å¤‡é€‰æ–¹æ¡ˆï¼šINT8_Per_Group_128/256**
   - ç²¾åº¦æ¥è¿‘ï¼ˆ50.60-50.80%ï¼‰
   - å†…å­˜å ç”¨æ›´ä½
   - é€‚åˆå¤§æ¨¡å‹éƒ¨ç½²

#### FP8 ä½¿ç”¨åœºæ™¯

1. **é€‚ç”¨åœºæ™¯**
   - å¯¹ç²¾åº¦è¦æ±‚ç¨ä½ï¼ˆå¯æ¥å— 1-2% æŸå¤±ï¼‰
   - éœ€è¦æ›´é«˜ç†è®ºååï¼ˆæœªæ¥ç¡¬ä»¶ä¼˜åŒ–ï¼‰
   - é…åˆ SmoothQuant ä½¿ç”¨

2. **æ³¨æ„äº‹é¡¹**
   - éœ€è¦æ›´ç»†ç²’åº¦çš„é‡åŒ–ï¼ˆgroup_size â‰¤ 128ï¼‰
   - RTX4090 ä¸Šå­˜åœ¨ç²¾åº¦é—®é¢˜ï¼Œéœ€æµ‹è¯•éªŒè¯
   - å»ºè®®åœ¨ Ada/Hopper æ¶æ„ä¸Šè°¨æ…ä½¿ç”¨

#### Fake Quantization ä½¿ç”¨åœºæ™¯

1. **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰**
   - è®­ç»ƒæ—¶ä½¿ç”¨ fake quantization æ¨¡æ‹Ÿé‡åŒ–è¯¯å·®
   - ä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€‚åº”é‡åŒ–
   - è®­ç»ƒåè½¬æ¢ä¸º real quantization

2. **PPL ä¼˜åŒ–åœºæ™¯**
   - Fake quantization çš„ PPL æ™®éä¼˜äº BF16ï¼ˆé™ä½ 13-17%ï¼‰
   - å¯èƒ½çš„æ­£åˆ™åŒ–æ•ˆåº”æœ‰åŠ©äºæ³›åŒ–
   - é€‚ç”¨äºå¯¹å›°æƒ‘åº¦æ•æ„Ÿçš„åº”ç”¨

3. **ç²¾åº¦ç ”ç©¶**
   - ç ”ç©¶é‡åŒ–è¯¯å·®æœ¬èº«å¯¹æ¨¡å‹çš„å½±å“
   - ä½œä¸º real quantization çš„ç†è®ºä¸Šç•Œ
   - åˆ†æä¸åŒé‡åŒ–ç²’åº¦çš„ç²¾åº¦æŸå¤±

4. **æ³¨æ„äº‹é¡¹**
   - Fake quantization ä¸æä¾›å®é™…çš„å†…å­˜å’Œè®¡ç®—ä¼˜åŠ¿
   - æƒé‡ä»ä»¥ BF16 å­˜å‚¨ï¼Œæ— æ³•å‡å°æ¨¡å‹ä½“ç§¯
   - æ¨ç†æ—¶ä»ä½¿ç”¨ BF16 è®¡ç®—ï¼Œæ— æ³•åŠ é€Ÿ

#### ç³»ç»Ÿä¼˜åŒ–æ–¹å‘

1. **Kernel Fusion**
   - **Per-Tensor**ï¼šæœ€å®¹æ˜“å®ç°ï¼Œå•ä¸€ scaleï¼Œå¯å®Œå…¨ fuse
   - **Per-Row**ï¼šä¸­ç­‰å¤æ‚åº¦ï¼ŒåŠ¨æ€åŠ è½½ scaleï¼Œå¯éƒ¨åˆ† fuse
   - **Per-Group**ï¼šæœ€å¤æ‚ï¼Œéœ€è¦é¢å¤–ç´¢å¼•è®¡ç®—ï¼Œéš¾ä»¥å®Œå…¨ fuse

2. **å†…å­˜ä¼˜åŒ–**
   - ä½¿ç”¨ FP16/BF16 å­˜å‚¨ scalesï¼Œå‡å°‘å†…å­˜å ç”¨
   - Per-Group çš„ scales å¯ä»¥å‹ç¼©å­˜å‚¨
   - è€ƒè™‘æ··åˆç²¾åº¦ç­–ç•¥ï¼ˆå…³é”®å±‚ FP16ï¼Œå…¶ä»– INT8ï¼‰

3. **æ¨ç†ä¼˜åŒ–**
   - æ‰¹å¤„ç†å¤§å°è°ƒæ•´ï¼šPer-Row æ”¯æŒæ›´å¤§ batch
   - é¢„è®¡ç®—é‡åŒ–å‚æ•°ï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€
   - ä½¿ç”¨ Tensor Core åŠ é€ŸçŸ©é˜µä¹˜

### 6.4 æœªæ¥å·¥ä½œ

1. **FP8 ä¼˜åŒ–**
   - ç ”ç©¶æ›´ç»†ç²’åº¦çš„é‡åŒ–ç­–ç•¥
   - æ¢ç´¢è‡ªé€‚åº”é‡åŒ–æ–¹æ³•
   - é’ˆå¯¹ Ada æ¶æ„ä¼˜åŒ– FP8 Kernel

2. **æ··åˆç²¾åº¦**
   - ä¸åŒå±‚ä½¿ç”¨ä¸åŒç²¾åº¦
   - æ¿€æ´»æ•æ„Ÿå±‚ä½¿ç”¨é«˜ç²¾åº¦
   - æƒé‡æ•æ„Ÿå±‚ä½¿ç”¨ INT8/FP16

3. **è‡ªåŠ¨åŒ–é‡åŒ–**
   - è‡ªåŠ¨é€‰æ‹©æœ€ä½³é‡åŒ–ç²’åº¦
   - è‡ªåŠ¨è°ƒä¼˜é‡åŒ–å‚æ•°
   - ç«¯åˆ°ç«¯é‡åŒ–æµç¨‹

---

## 7. é™„å½•

### 7.1 å®éªŒæ•°æ®æ–‡ä»¶

æ‰€æœ‰å®éªŒç»“æœä¿å­˜åœ¨ `/datadisk/workspace/Quant/nano-vllm-hw3/results/` ç›®å½•ï¼š

```
results/
â”œâ”€â”€ Task 3.1: é‡åŒ–ç²¾åº¦æµ‹è¯•
â”‚   â”œâ”€â”€ task_31_ppl_*.json              # PPL ç»“æœ
â”‚   â”œâ”€â”€ task_31_mmlu_*_direct.json     # MMLU Direct ç»“æœ
â”‚   â”œâ”€â”€ task_31_mmlu_*_generate.json   # MMLU Generate ç»“æœ
â”‚   â”œâ”€â”€ task_31_ppl_*_Fake.json        # PPL Fake Quant ç»“æœ
â”‚   â”œâ”€â”€ task_31_mmlu_*_Fake_direct.json    # MMLU Fake Direct ç»“æœ
â”‚   â”œâ”€â”€ task_31_mmlu_*_Fake_generate.json  # MMLU Fake Generate ç»“æœ
â”‚   â””â”€â”€ task_31_results_*.json         # æ±‡æ€»ç»“æœ
â”‚
â”œâ”€â”€ Task 3.2: é‡åŒ–ååæµ‹è¯•
â”‚   â”œâ”€â”€ task_32_mmlu_fake_quant_results.json      # ä¼ªé‡åŒ– MMLU
â”‚   â””â”€â”€ task_32_results.json                     # ååç»“æœ
â”‚
â””â”€â”€ Task 3.3: SmoothQuant
    â”œâ”€â”€ task_33_int8_results.json              # INT8 å®Œæ•´ç»“æœ
    â”œâ”€â”€ task_33_fp8_results.json               # FP8 å®Œæ•´ç»“æœ
    â””â”€â”€ task_33_matmul_accuracy_results.json   # çŸ©é˜µä¹˜æ³•ç²¾åº¦
```

### 7.2 è¿è¡Œå‘½ä»¤

#### å•ç‹¬æµ‹è¯•

```bash
# Task 3.1 - é‡åŒ–ç²¾åº¦æµ‹è¯•ï¼ˆReal Quantizationï¼‰
conda run -n torch python task_31_ppl_test.py BF16
conda run -n torch python task_31_mmlu_test.py BF16 direct
conda run -n torch python task_31_mmlu_test.py BF16 generate

# Task 3.1 - é‡åŒ–ç²¾åº¦æµ‹è¯•ï¼ˆFake Quantizationï¼‰
conda run -n torch python task_31_ppl_fake_test.py INT8_Per_Row_Fake
conda run -n torch python task_31_mmlu_fake_test.py INT8_Per_Row_Fake direct
conda run -n torch python task_31_mmlu_fake_test.py INT8_Per_Row_Fake generate

# Task 3.2 - ä¼ªé‡åŒ–ç²¾åº¦æµ‹è¯•
conda run -n torch python task_32_mmlu_fake_quant_test.py
conda run -n torch python task_32_throughput_test.py

# Task 3.3 - SmoothQuant
python task_33_smoothquant.py --quant-type int8 --method both
python task_33_smoothquant.py --quant-type fp8 --method both
```

#### æ‰¹é‡è¿è¡Œ

```bash
# è¿è¡Œå®Œæ•´çš„ Task 3.1ï¼ˆReal Quantizationï¼‰
conda run -n torch python run_task_31.py

# è¿è¡Œå®Œæ•´çš„ Task 3.1ï¼ˆFake Quantizationï¼‰
conda run -n torch python run_task_31_fake.py

# è¿è¡Œå®Œæ•´çš„ Task 3.2
conda run -n torch python run_task_32.py

# è¿è¡Œå®Œæ•´çš„ Task 3.3
python run_task_33.py
```

### 7.3 å…³é”®ä»£ç æ–‡ä»¶

```
nano-vllm-hw3/
â”œâ”€â”€ nanovllm/
â”‚   â””â”€â”€ utils/quantization.py          # é‡åŒ–å®ç°ï¼ˆæ ¸å¿ƒï¼‰
â”‚       â”œâ”€â”€ fake_per_tensor_quant()    # Fake Per-Tensor é‡åŒ–
â”‚       â”œâ”€â”€ fake_per_row_quant()       # Fake Per-Row é‡åŒ–
â”‚       â”œâ”€â”€ fake_per_group_quant()     # Fake Per-Group é‡åŒ–
â”‚       â”œâ”€â”€ apply_weight_fake_quant()  # åº”ç”¨ Fake æƒé‡é‡åŒ–
â”‚       â”œâ”€â”€ per_tensor_quant()         # Real Per-Tensor é‡åŒ–
â”‚       â”œâ”€â”€ per_row_quant()            # Real Per-Row é‡åŒ–
â”‚       â””â”€â”€ per_group_quant()          # Real Per-Group é‡åŒ–
â”œâ”€â”€ nanovllm/models/qwen3.py             # Qwen3 æ¨¡å‹
â”œâ”€â”€ task_31_ppl_test.py                 # Task 3.1 PPL æµ‹è¯•ï¼ˆRealï¼‰
â”œâ”€â”€ task_31_mmlu_test.py                # Task 3.1 MMLU æµ‹è¯•ï¼ˆRealï¼‰
â”œâ”€â”€ task_31_ppl_fake_test.py            # Task 3.1 PPL æµ‹è¯•ï¼ˆFakeï¼‰
â”œâ”€â”€ task_31_mmlu_fake_test.py           # Task 3.1 MMLU æµ‹è¯•ï¼ˆFakeï¼‰
â”œâ”€â”€ run_task_31.py                       # Task 3.1 æ‰¹é‡è¿è¡Œï¼ˆRealï¼‰
â”œâ”€â”€ run_task_31_fake.py                  # Task 3.1 æ‰¹é‡è¿è¡Œï¼ˆFakeï¼‰
â”œâ”€â”€ task_32_mmlu_fake_quant_test.py     # Task 3.2 ä¼ªé‡åŒ– MMLU
â”œâ”€â”€ task_32_throughput_test.py          # Task 3.2 ååæµ‹è¯•
â”œâ”€â”€ task_33_smoothquant.py              # Task 3.3 SmoothQuant
â”œâ”€â”€ run_task_32.py                       # Task 3.2 æ‰¹é‡è¿è¡Œ
â””â”€â”€ run_task_33.py                       # Task 3.3 æ‰¹é‡è¿è¡Œ
```

### 7.4 å®éªŒç¯å¢ƒé…ç½®

```bash
# æ¿€æ´» conda ç¯å¢ƒï¼ˆéœ€è¦ flash_attnï¼‰
conda activate torch

# æˆ–ä½¿ç”¨ conda run
conda run -n torch <command>

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH=/datadisk/workspace/Quant/nano-vllm-hw3:$PYTHONPATH
export HF_ENDPOINT=https://hf-mirror.com

# å®‰è£…ä¾èµ–ï¼ˆå¦‚éœ€è¦ï¼‰
pip install torchao
```

---

**æŠ¥å‘Šå®Œæˆæ—¥æœŸï¼š** 2024å¹´12æœˆ30æ—¥
**å®éªŒå®Œæˆæ—¶é—´ï¼š** çº¦ 4 å°æ—¶
**æµ‹è¯•é…ç½®æ€»æ•°ï¼š** 13 ç§é‡åŒ–é…ç½® Ã— 2 ç§æ–¹æ³•ï¼ˆdirect/generateï¼‰
**ç”Ÿæˆç»“æœæ–‡ä»¶ï¼š** 50+ JSON æ–‡ä»¶
